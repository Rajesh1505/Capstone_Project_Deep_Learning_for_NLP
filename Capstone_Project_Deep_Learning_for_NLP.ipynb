{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - IndustryGPT: Specialized LLM Bot Using Pre-Trained Models\n"
      ],
      "metadata": {
        "id": "GXc66GfM0yhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Deep Learning for NLP\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Rajesh Kumar Patel"
      ],
      "metadata": {
        "id": "fA3XvK5I0yhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GitHub Link -** https://github.com/Rajesh1505/Capstone_Project_Deep_Learning_for_NLP.git"
      ],
      "metadata": {
        "id": "4KVSAu1o0yhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Title:\n",
        "**IndustryGPT: Specialized LLM Bot Using Pre-Trained Models**\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this capstone project, We will embark on an exciting journey to create an Industry-Specific Large Language Model (LLM) Bot using state-of-the-art pre-trained models from sources like Hugging Face. The primary objective is to build an intelligent bot that can effectively engage with users by answering questions and providing insights specific to a chosen industry. This project will not only enhance our technical skills but also provide a deep understanding of the chosen industry's nuances, challenges, and trends.\n",
        "\n",
        "## Project Background\n",
        "\n",
        "With the rapid advancement of artificial intelligence and machine learning, Large Language Models (LLMs) have become pivotal in transforming various industries by automating and enhancing communication. These models, especially those leveraging architectures like GPT-3 and beyond, are capable of understanding and generating human-like text, making them ideal for creating intelligent conversational agents.\n",
        "\n",
        "This capstone project focuses on harnessing the power of pre-trained LLMs from platforms like Hugging Face to develop industry-specific bots. Students are given the freedom to choose from a diverse array of industries, each presenting unique challenges and opportunities for applying LLM technology. The selected industry will guide the data collection process, ensuring that the bot is trained on relevant and specific information to enhance its contextual understanding and response accuracy.\n",
        "\n",
        "By engaging in this project, we will not only learn to fine-tune pre-trained models but also gain hands-on experience in handling real-world data. The project is designed to be manageable even for those with limited computational resources. The primary objective is to build a bot that can think and engage with users effectively, providing coherent and contextually appropriate answers.\n",
        "\n",
        "This project serves as a foundation for the Industry Immersion module, where students will extend their work into a comprehensive research paper, delving deeper into the chosen industry's specific applications and implications of LLM technology. This holistic approach ensures that students are well-prepared to tackle real-world challenges using cutting-edge AI technologies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Project Goal\n",
        "\n",
        "The primary goal of this capstone project is to develop an industry-specific Large Language Model (LLM) Bot using pre-trained models from platforms such as Hugging Face. Students will be tasked with selecting one industry from a provided list, gathering relevant data, fine-tuning a pre-trained LLM, and demonstrating the bot's capability to engage users effectively by providing accurate and contextually appropriate responses."
      ],
      "metadata": {
        "id": "VNYSNB_80yhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "%pip install huggingface_hub\n",
        "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.8.3\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:05:52.206743Z",
          "iopub.execute_input": "2025-02-06T09:05:52.207033Z",
          "iopub.status.idle": "2025-02-06T09:05:55.76271Z",
          "shell.execute_reply.started": "2025-02-06T09:05:52.206997Z",
          "shell.execute_reply": "2025-02-06T09:05:55.76164Z"
        },
        "id": "x2S2CeXs0yhM",
        "outputId": "acaa9ad8-69ce-484f-9a9a-595510c678a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Note: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:06:08.845996Z",
          "iopub.execute_input": "2025-02-06T09:06:08.846328Z",
          "iopub.status.idle": "2025-02-06T09:06:33.711608Z",
          "shell.execute_reply.started": "2025-02-06T09:06:08.846299Z",
          "shell.execute_reply": "2025-02-06T09:06:33.710902Z"
        },
        "id": "KODQQMPh0yhN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:06:41.633866Z",
          "iopub.execute_input": "2025-02-06T09:06:41.634158Z",
          "iopub.status.idle": "2025-02-06T09:06:41.637845Z",
          "shell.execute_reply.started": "2025-02-06T09:06:41.634137Z",
          "shell.execute_reply": "2025-02-06T09:06:41.636911Z"
        },
        "id": "hljF7cyj0yhN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "dataset1 = load_dataset(\"Rajesh1505/finance-alpaca-1k-test\")\n",
        "dataset2 = load_dataset(\"Rajesh1505/alpaca_finance_en\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:07:39.652159Z",
          "iopub.execute_input": "2025-02-06T09:07:39.652509Z",
          "iopub.status.idle": "2025-02-06T09:07:45.422556Z",
          "shell.execute_reply.started": "2025-02-06T09:07:39.652477Z",
          "shell.execute_reply": "2025-02-06T09:07:45.421889Z"
        },
        "id": "3gKERqey0yhN",
        "outputId": "661f5d5d-013f-43b6-9292-10849629e80c",
        "colab": {
          "referenced_widgets": [
            "1817b3bc2f824434ac029bfa2da4d140",
            "127d2eb3f91f4c1f9665aba533fb7c40",
            "9aadad5eedb44d35b61050953b1bd72e",
            "1820989b02bf4689aa233725281ef539",
            "c936063c2d0c4d908cdd648ca6832563",
            "26337472c94845ce99714996a5357ade"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/381 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1817b3bc2f824434ac029bfa2da4d140"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/665k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "127d2eb3f91f4c1f9665aba533fb7c40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9aadad5eedb44d35b61050953b1bd72e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/384 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1820989b02bf4689aa233725281ef539"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/23.7M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c936063c2d0c4d908cdd648ca6832563"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/68912 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26337472c94845ce99714996a5357ade"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess datasets\n",
        "def combine_text_columns(example):\n",
        "    return {'text': f\"{example['instruction']} ### {example['output']}\"}\n",
        "\n",
        "dataset1 = dataset1.map(combine_text_columns)\n",
        "dataset2 = dataset2.map(combine_text_columns)\n",
        "\n",
        "# Remove unused columns\n",
        "dataset1['test'] = dataset1['test'].remove_columns(['instruction', 'input', 'output'])\n",
        "dataset2['train'] = dataset2['train'].remove_columns(['instruction', 'input', 'output', 'id'])\n",
        "\n",
        "# Split datasets\n",
        "split_dataset1 = dataset1['test'].train_test_split(train_size=0.8)\n",
        "split_dataset2 = dataset2['train'].train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:08:10.406334Z",
          "iopub.execute_input": "2025-02-06T09:08:10.406642Z",
          "iopub.status.idle": "2025-02-06T09:08:15.032516Z",
          "shell.execute_reply.started": "2025-02-06T09:08:10.406619Z",
          "shell.execute_reply": "2025-02-06T09:08:15.031813Z"
        },
        "id": "KlIabptH0yhO",
        "outputId": "f59ae3e3-17c8-49c8-ec02-016bff527008",
        "colab": {
          "referenced_widgets": [
            "9e902f8588064d20bbea0b65ef1a8430",
            "243fc2ef37a440d693dc4b658748e355"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e902f8588064d20bbea0b65ef1a8430"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/68912 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "243fc2ef37a440d693dc4b658748e355"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge datasets\n",
        "merged_train = concatenate_datasets([split_dataset1['train'], split_dataset2['train']])\n",
        "merged_test = concatenate_datasets([split_dataset1['test'], split_dataset2['test']])\n",
        "\n",
        "merged_dataset = DatasetDict({'train': merged_train, 'test': merged_test})\n",
        "\n",
        "\n",
        "merged_train_dataset = merged_dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "merged_test_dataset = merged_dataset['test'].shuffle(seed=42).select(range(100))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:08:18.394062Z",
          "iopub.execute_input": "2025-02-06T09:08:18.394405Z",
          "iopub.status.idle": "2025-02-06T09:08:18.435904Z",
          "shell.execute_reply.started": "2025-02-06T09:08:18.394374Z",
          "shell.execute_reply": "2025-02-06T09:08:18.435015Z"
        },
        "id": "2kFa_FGR0yhO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Llama format\n",
        "def transform_conversation(example):\n",
        "    segments = example['text'].split('###')\n",
        "    reformatted_segments = []\n",
        "    for i in range(0, len(segments) - 1, 2):\n",
        "        prompt = segments[i].strip()\n",
        "        answer = segments[i + 1].strip() if i + 1 < len(segments) else \"\"\n",
        "        reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "transformed_train_dataset = merged_train_dataset.map(transform_conversation)\n",
        "transformed_test_dataset = merged_test_dataset.map(transform_conversation)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:08:23.304449Z",
          "iopub.execute_input": "2025-02-06T09:08:23.30478Z",
          "iopub.status.idle": "2025-02-06T09:08:23.742924Z",
          "shell.execute_reply.started": "2025-02-06T09:08:23.304754Z",
          "shell.execute_reply": "2025-02-06T09:08:23.742034Z"
        },
        "id": "T0dsFXGY0yhO",
        "outputId": "f91ae82a-374d-4e5b-97a4-432c1c6869e1",
        "colab": {
          "referenced_widgets": [
            "22230de99fdf4b39b6482990262deafc",
            "d30b9e700a6e493c8d0c546fa87d8107"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22230de99fdf4b39b6482990262deafc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d30b9e700a6e493c8d0c546fa87d8107"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Llama-2 model\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "new_model = \"llama_2_7b_finance_finetune_model\"\n",
        "\n",
        "# Configure Quantization (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:08:27.357251Z",
          "iopub.execute_input": "2025-02-06T09:08:27.357564Z",
          "iopub.status.idle": "2025-02-06T09:08:27.363075Z",
          "shell.execute_reply.started": "2025-02-06T09:08:27.357539Z",
          "shell.execute_reply": "2025-02-06T09:08:27.362218Z"
        },
        "id": "Q0TaqcQp0yhO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model with CPU Offloading\n",
        "device_map = \"auto\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    offload_folder=\"/kaggle/working\"  # Offload to disk to prevent OOM\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:08:34.385182Z",
          "iopub.execute_input": "2025-02-06T09:08:34.385536Z",
          "iopub.status.idle": "2025-02-06T09:09:56.18124Z",
          "shell.execute_reply.started": "2025-02-06T09:08:34.38551Z",
          "shell.execute_reply": "2025-02-06T09:09:56.180562Z"
        },
        "id": "zZLyecoE0yhP",
        "outputId": "eea6f26f-ab10-4b63-ff32-2d614559625f",
        "colab": {
          "referenced_widgets": [
            "527dc39cd4ad428d908d91e81c2103cb",
            "01dc0df6a4024f49831602c4ff18feb8",
            "3392547738174660a5482ab23802c0fa",
            "2381ec27405242a18f4ce6487900d12b",
            "476e5a75e11f4a87bb4f6cc6d17a755d",
            "39139f5caf71428b98af716c95f00627",
            "c500faf365ae438393171666cde1c5e6"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "527dc39cd4ad428d908d91e81c2103cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01dc0df6a4024f49831602c4ff18feb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3392547738174660a5482ab23802c0fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2381ec27405242a18f4ce6487900d12b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "476e5a75e11f4a87bb4f6cc6d17a755d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39139f5caf71428b98af716c95f00627"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c500faf365ae438393171666cde1c5e6"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# LoRA Configuration (Memory Efficient Fine-Tuning)\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:09:59.495727Z",
          "iopub.execute_input": "2025-02-06T09:09:59.496028Z",
          "iopub.status.idle": "2025-02-06T09:10:01.093999Z",
          "shell.execute_reply.started": "2025-02-06T09:09:59.496006Z",
          "shell.execute_reply": "2025-02-06T09:10:01.093291Z"
        },
        "id": "eeoxr0d90yhP",
        "outputId": "7211dad1-658f-45d9-a680-342339c94ec2",
        "colab": {
          "referenced_widgets": [
            "ad198f01298c461caeafbc5f991e58ea",
            "6a176b4bdc65437eb0bed88315a15e54",
            "1e4b75451ce24ba881ae1006577c913c",
            "4d0b90290e0b4a80a423e38bbacf6aaa",
            "076ebcf67fb543058af6773e1b608371"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad198f01298c461caeafbc5f991e58ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a176b4bdc65437eb0bed88315a15e54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e4b75451ce24ba881ae1006577c913c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d0b90290e0b4a80a423e38bbacf6aaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "076ebcf67fb543058af6773e1b608371"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Arguments (Optimized for Low VRAM)\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=0,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:10:07.388173Z",
          "iopub.execute_input": "2025-02-06T09:10:07.388543Z",
          "iopub.status.idle": "2025-02-06T09:10:07.394381Z",
          "shell.execute_reply.started": "2025-02-06T09:10:07.388515Z",
          "shell.execute_reply": "2025-02-06T09:10:07.393703Z"
        },
        "id": "zPvzNa2m0yhP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=transformed_train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=350,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T09:10:15.614029Z",
          "iopub.execute_input": "2025-02-06T09:10:15.614376Z",
          "iopub.status.idle": "2025-02-06T10:21:33.086135Z",
          "shell.execute_reply.started": "2025-02-06T09:10:15.614345Z",
          "shell.execute_reply": "2025-02-06T10:21:33.085278Z"
        },
        "id": "pSCK09Ht0yhP",
        "outputId": "254effc5-397f-4c65-fb21-57b25fb20a50",
        "colab": {
          "referenced_widgets": [
            "1c14067f8eac42b0829f337b01fe4174"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c14067f8eac42b0829f337b01fe4174"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 1:10:51, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.387900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.886700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.805600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.530900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.797200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.508100</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.738600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.442900</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.671900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.444800</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.721800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.437300</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.743600</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.387900</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.692300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.492200</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.642800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.416000</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.725100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.403700</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.734800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.410500</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.739400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.383200</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.519700</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=625, training_loss=1.6266033935546875, metrics={'train_runtime': 4265.6466, 'train_samples_per_second': 1.172, 'train_steps_per_second': 0.147, 'total_flos': 1.258110305550336e+16, 'train_loss': 1.6266033935546875, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "tokenizer.save_pretrained(new_model)\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T10:22:00.262736Z",
          "iopub.execute_input": "2025-02-06T10:22:00.263029Z",
          "iopub.status.idle": "2025-02-06T10:22:00.522201Z",
          "shell.execute_reply.started": "2025-02-06T10:22:00.262999Z",
          "shell.execute_reply": "2025-02-06T10:22:00.521348Z"
        },
        "id": "PVi9h9u80yhP",
        "outputId": "c7e670e8-78d1-4b20-f1c0-780f57f39e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model saved successfully!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Model with Text Generation\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "# prompt = \"Analyze potential financial risks based on current market conditions.\"\n",
        "prompt = \"Summary of investment portfolio optimization.\"\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T10:22:46.082365Z",
          "iopub.execute_input": "2025-02-06T10:22:46.082686Z",
          "iopub.status.idle": "2025-02-06T10:23:59.578498Z",
          "shell.execute_reply.started": "2025-02-06T10:22:46.082658Z",
          "shell.execute_reply": "2025-02-06T10:23:59.577581Z"
        },
        "id": "JSIbWTup0yhP",
        "outputId": "bd324a99-cb07-4db0-c369-cf87e7534d72"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<s>[INST] Summary of investment portfolio optimization. [/INST] Investment portfolio optimization is the process of selecting the optimal mix of assets to achieve a desired investment outcome. It involves analyzing the risk and return of different assets, and selecting the optimal mix of assets to achieve the desired investment outcome. The optimal mix of assets is determined by the investor's risk tolerance, investment horizon, and investment goals. The optimal mix of assets is also influenced by the investor's tax situation, investment costs, and other factors. The optimal mix of assets is typically a combination of low-risk assets, such as bonds, and high-risk assets, such as stocks. The optimal mix of assets is also influenced by the investor's investment horizon, which is the time frame in which the investor plans to achieve their investment goals. The optimal mix of assets is also influenced by the investor\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Push Model to Hugging Face Hub\n",
        "from huggingface_hub import login\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# Retrieve Hugging Face token\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"hf_api_key\")\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=hf_token)\n",
        "\n",
        "# Push model and tokenizer\n",
        "model_repo_name = \"Rajesh1505/llama_2_7b_finance_finetune_model\"\n",
        "model.push_to_hub(model_repo_name)\n",
        "tokenizer.push_to_hub(model_repo_name)\n",
        "\n",
        "# Free GPU Memory\n",
        "torch.cuda.empty_cache()\n",
        "print(\"llama_2_7b_finance_finetune_model pushed to Hugging Face Hub.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T10:33:31.917922Z",
          "iopub.execute_input": "2025-02-06T10:33:31.918262Z",
          "iopub.status.idle": "2025-02-06T10:33:31.92312Z",
          "shell.execute_reply.started": "2025-02-06T10:33:31.918195Z",
          "shell.execute_reply": "2025-02-06T10:33:31.922312Z"
        },
        "id": "-cF2cXSN0yhP",
        "outputId": "af6a1049-6f9b-4f18-a04e-8a672acb7034"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "llama_2_7b_finance_finetune_model pushed to Hugging Face Hub.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "-smJZVil0yhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T10:27:03.893978Z",
          "iopub.execute_input": "2025-02-06T10:27:03.894307Z",
          "iopub.status.idle": "2025-02-06T10:27:14.740894Z",
          "shell.execute_reply.started": "2025-02-06T10:27:03.89428Z",
          "shell.execute_reply": "2025-02-06T10:27:14.7398Z"
        },
        "id": "zllF1OYb0yhQ",
        "outputId": "5be500d3-8b1a-4172-c64a-58305c7b81c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load the fine-tuned model from Hugging Face\n",
        "model_name = \"Rajesh1505/llama_2_7b_finance_finetune_model\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# # Load tokenizer and model\n",
        "tokenizer_ = AutoTokenizer.from_pretrained(model_name)\n",
        "model_ = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Load pipeline for text generation\n",
        "pipe = pipeline(\"text-generation\", model=model_, tokenizer=tokenizer_, max_length=200)\n",
        "\n",
        "def chatbot_response(prompt):\n",
        "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "    response = pipe(formatted_prompt)[0]['generated_text']\n",
        "    return response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "# Gradio UI\n",
        "app_gr = gr.Interface(\n",
        "    fn=chatbot_response,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Ask a finance-related question...\"),\n",
        "    outputs=gr.Textbox(label=\"Finance Chatbot Response\"),\n",
        "    title=\"Finance LLM Chatbot\",\n",
        "    description=\"Ask finance-related questions and get AI-powered responses!\"\n",
        ")\n",
        "\n",
        "\n",
        "app_gr.launch(debug=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T10:28:39.525396Z",
          "iopub.execute_input": "2025-02-06T10:28:39.525699Z",
          "iopub.status.idle": "2025-02-06T10:33:05.24394Z",
          "shell.execute_reply.started": "2025-02-06T10:28:39.525677Z",
          "shell.execute_reply": "2025-02-06T10:33:05.243044Z"
        },
        "id": "MNZqQoZt0yhQ",
        "outputId": "de5cce18-cd8d-4425-de45-67e9a33e387f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://1d58820c1fb2e40130.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://1d58820c1fb2e40130.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://1d58820c1fb2e40130.gradio.live\n",
          "output_type": "stream"
        },
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "mrwu6HJN0yhQ"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}